{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Filesystem\n",
    "This notebook is primarily meant to be executed in Colab as a computational backend. If you want to run on your own hardware with data, you need to set `data_dir` and `ALLOW_IO`\n",
    "\n",
    "This notebook viewable directly on Colab from [https://colab.research.google.com/github/rcharan/phutball/blob/rl/pytorch-implementation/model-training.ipynb](https://colab.research.google.com/github/rcharan/phutball/blob/rl/pytorch-implementation/model-training.ipynb) (it is a mirror of github). But if it has moved branches or you are looking at a past commit, look at the [Google instructions](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb) on where to find this file.\n",
    "\n",
    "The workflow is:\n",
    " - Data stored in (my personal/private) Google Drive\n",
    " - Utilities/library files (for importing) on github, edited on local hardware and pushed to github.\n",
    " - Notebook hosted on github, edited both in Colab or locally (depending on the relative value of having a GPU attached versus being able to use regular Jupyter keyboard shortcuts/a superior interface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RY-nqnzUzB5k"
   },
   "outputs": [],
   "source": [
    "# Attempt Colab setup if on Colab\n",
    "try:\n",
    "  import google.colab\n",
    "except:\n",
    "  ALLOW_IO = False\n",
    "else:\n",
    "  # Mount Google Drive at data_dir\n",
    "  #  (for data)\n",
    "  from google.colab import drive\n",
    "  from os.path import join\n",
    "  ROOT = '/content/drive'\n",
    "  DATA = 'My Drive/phutball'\n",
    "  drive.mount(ROOT)\n",
    "  ALLOW_IO = True\n",
    "  data_dir = join(ROOT, DATA)\n",
    "  !mkdir \"{data_dir}\"     # in case we haven't created it already   \n",
    "\n",
    "  # Pull in code from github\n",
    "  %cd /content\n",
    "  github_repo = 'https://github.com/rcharan/phutball'\n",
    "  !git clone -b rl {github_repo}\n",
    "  %cd /content/phutball\n",
    "  \n",
    "  # Point python to code base\n",
    "  import sys\n",
    "  sys.path.append('/content/phutball/pytorch-implementation')\n",
    "\n",
    "  # Updater for library functions changed on local hardware and pushed to github\n",
    "  #  (circuitous, I know)\n",
    "  def update_repo():\n",
    "    !git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yF7BrTwZwUz_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jbaorYhzwUze"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "# Codebase\n",
    "from lib.model_v1          import TDConway\n",
    "from lib.moves             import create_placement_getter, get_jumps\n",
    "from lib.utilities         import config, lfilter\n",
    "from lib.testing_utilities import create_state, visualize_state, boards\n",
    "from lib.timer             import Timer\n",
    "from lib.moves             import END_LOC, COL, CHAIN\n",
    "\n",
    "# Graphics for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "plt.ioff()\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "# Tensorflow solely for the Progress Bar (totally worth it)\n",
    "from tensorflow.keras.utils import Progbar as ProgressBar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Management Utilities\n",
    "Setup for GPU, CPU, or (not working well/fully implemented) TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.00GiB available memory on CPU-based runtime\n"
     ]
    }
   ],
   "source": [
    "use_tpu = False\n",
    "\n",
    "if use_tpu:\n",
    "  # Install PyTorch/XLA\n",
    "  !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
    "  !python pytorch-xla-env-setup.py --version $VERSION\n",
    "  import torch_xla\n",
    "  import torch_xla.core.xla_model as xm\n",
    "  \n",
    "  # Set the device\n",
    "  device = xm.xla_device()\n",
    "  \n",
    "  # Memory inspection\n",
    "  def print_memory_usage():\n",
    "    print('TPU memory inspection not implemented')\n",
    "  def print_max_memory_usage():\n",
    "    print('TPU memory inspection not implemented')\n",
    "  def garbage_collect():\n",
    "    gc.collect() # No TPU specific implementation yet\n",
    "    \n",
    "elif torch.cuda.is_available():\n",
    "  # Set the device\n",
    "  device = torch.device('cuda')\n",
    "  \n",
    "  # Echo GPU info\n",
    "  gpu_info = !nvidia-smi\n",
    "  gpu_info = '\\n'.join(gpu_info)\n",
    "  print(gpu_info)\n",
    "  \n",
    "  # Memory inspection and management\n",
    "  from lib.memory import (\n",
    "    print_memory_usage_cuda     as print_memory_usage,\n",
    "    print_max_memory_usage_cuda as print_max_memory_usage,\n",
    "    garbage_collect_cuda        as garbage_collect\n",
    "  )\n",
    "\n",
    "else:\n",
    "  # Set the device to CPU\n",
    "  device = torch.device('cpu')\n",
    "  \n",
    "  # Echo RAM info\n",
    "  from psutil import virtual_memory\n",
    "  from lib.memory import format_bytes\n",
    "  ram = virtual_memory().total\n",
    "  print(format_bytes(ram), 'available memory on CPU-based runtime')\n",
    "  \n",
    "  # Memory inspection and management\n",
    "  from lib.memory import (\n",
    "    print_memory_usage, \n",
    "    print_max_memory_usage,\n",
    "    garbage_collect\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kl9_Q1WNwUz8"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "MAX_JUMPS = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tools\n",
    "These tools will be moved to .py files in the lib folder when stable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rkzXSlsbwU0C"
   },
   "outputs": [],
   "source": [
    "class AlternatingTDLambda(Optimizer):\n",
    "  '''Implements tracing and updates for the TD(λ) algorithm.\n",
    "  \n",
    "  For details see Sutton and Barto, Reinforcement Learning 2ed,\n",
    "  Chaper 12 Section 2.\n",
    "  \n",
    "  Modifications:\n",
    "    (1) the algorithm is modified to compute traces\n",
    "        one step early (when the graph is available). Hence, trace must be\n",
    "        initialized with update_trace before calling step.\n",
    "        \n",
    "    (2) Because the board switches sides each turn, eligibility trace updates\n",
    "        must be of the form z_t+1 <-  -λz_t + 𝝯v\n",
    "  \n",
    "  You may find the following greeks a helpful reference:\n",
    "  alpha : Learning Rate\n",
    "  lambda: Exponential Decay parameter for the eligibility trace\n",
    "          (which is essentially momentum but with different\n",
    "           interpertation and is occasionally zeroed out).\n",
    "  delta : Temporal difference (TD) i.e. the difference between the estimated\n",
    "          value of a step and the realized value upon the best move (based\n",
    "          on further estimation of course).\n",
    "  '''\n",
    "  def __init__(self, parameters, alpha, lamda):\n",
    "    self.alpha = alpha\n",
    "    self.lamda = lamda # Note alternate spelling (I didn't make it up!)\n",
    "    defaults   = dict(alpha = alpha, lamda = lamda)\n",
    "\n",
    "    super(TDLambda, self).__init__(parameters, defaults)\n",
    "    \n",
    "  @torch.no_grad()\n",
    "  def step(self, delta, update_trace = True):\n",
    "    '''Performs a single optimization step updating the trace *afterwards*\n",
    "    \n",
    "    update_trace must be called before first step to initialize the trace.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    \n",
    "    delta: Difference between estimated value of move at time t and realized\n",
    "           value after moving and going to time t+1\n",
    "    '''\n",
    "    \n",
    "    for group in self.param_groups:\n",
    "      alpha = group['alpha']\n",
    "      \n",
    "      for p in group['params']:\n",
    "        if p.grad is None:\n",
    "          continue\n",
    "        \n",
    "        state = self.state[p]\n",
    "        \n",
    "        if len(state) == 0 or 'trace' not in state:\n",
    "          raise RuntimeError('Traces must be initialized before calling step')\n",
    "          \n",
    "        trace = state['trace']\n",
    "        \n",
    "        p.add_(alpha * delta * trace) # Note gradient *ascent* in reinforcement learning\n",
    "    \n",
    "    if update_trace:\n",
    "      self._update_trace()\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def _update_trace(self):\n",
    "    '''Updates the trace based on the gradients.\n",
    "    \n",
    "    This also is the only way to initialize the traces.\n",
    "    It must be called after evaluating the starting position\n",
    "    and backprop at the beginning of the game, usually from\n",
    "    the optimizers restart method.\n",
    "    '''\n",
    "    for group in self.param_groups:\n",
    "      lamda = group['lamda']\n",
    "      \n",
    "      for p in group['params']:\n",
    "        if p.grad is None:\n",
    "          continue\n",
    "        \n",
    "        state = self.state[p]\n",
    "        \n",
    "        # Initialize to zero if necessary\n",
    "        if len(state) == 0 or 'trace' not in state:\n",
    "          state['trace'] = torch.zeros_like(p)\n",
    "          \n",
    "        trace = state['trace']\n",
    "        trace.mul_(-lamda).add_(p.grad)\n",
    "      \n",
    "  @torch.no_grad()\n",
    "  def _zero_trace(self):\n",
    "    for group in self.param_groups:\n",
    "      for p in group['params']:\n",
    "        state = self.state['p']\n",
    "        if 'trace' not in state:\n",
    "          continue\n",
    "          \n",
    "        trace = state['trace']\n",
    "        trace.zero_()\n",
    "    \n",
    "  def restart(self, score):\n",
    "    '''Call to restart the trace\n",
    "    \n",
    "    Returns score.item() as a convenience\n",
    "    '''\n",
    "    self._zero_trace()\n",
    "    score.backwards()\n",
    "    self.update_trace()\n",
    "    self.zero_grad()\n",
    "    return score.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p1CAfi-4wU0H"
   },
   "outputs": [],
   "source": [
    "def training_loop(optimizer, num_games, off_policy = lambda _ : None):\n",
    "  \n",
    "  initial_state = create_state('H10').to(device)\n",
    "  for i in range(num_games):\n",
    "    print(f'\\nPlaying game {i+1} of {num_games}:')\n",
    "    game_loop(initial_state, model, optimizer, off_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "62GkFibrwU0Q"
   },
   "outputs": [],
   "source": [
    "def game_loop(initial_state, model, optimizer, off_policy):\n",
    "  '''Training loop that plays one game'''\n",
    "  # Just in case\n",
    "  optimizer.zero_grad()\n",
    "  \n",
    "  # Initialization\n",
    "  state    = initial_state\n",
    "  score, _ = model(state.unsqueeze(0))\n",
    "  v_t      = optimizer.restart(score)\n",
    "  \n",
    "  # Progress Bar\n",
    "  bar      = ProgressBar(284)\n",
    "  move_num = 1\n",
    "  \n",
    "  while True:\n",
    "    # Determine the next move\n",
    "    game_over, moved_off_policy, new_state, score = \\\n",
    "      get_next_move_training(curr_state, off_policy = off_policy)\n",
    "    \n",
    "    if game_over:      \n",
    "      delta = 1 - v_t\n",
    "      optimizer.step(delta, update_trace = False)\n",
    "      \n",
    "      # Terminate the progress bar\n",
    "      bar.target = move_num\n",
    "      bar.update(move_num)\n",
    "\n",
    "      break\n",
    "    \n",
    "    elif moved_off_policy:\n",
    "      # Equivalent to starting a new game\n",
    "      v_t = restart(optimizer, score)\n",
    "      \n",
    "    else:\n",
    "      score.backwards()\n",
    "      delta = (1 - score) - v_t\n",
    "      optimizer.step(delta)\n",
    "      optimizer.zero_grad()\n",
    "      \n",
    "      v_t   = score.item()\n",
    "      state = new_state\n",
    "      \n",
    "    # Progress bar\n",
    "    move_num += 1\n",
    "    if move_num >= bar.target * 0.9:\n",
    "      bar.target += bar.target // 10 + 1\n",
    "    \n",
    "    bar.update(move_num)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2_Kl9lNTwU0T"
   },
   "outputs": [],
   "source": [
    "def get_next_move_training(curr_state, off_policy = lambda _ : None, profile = 0):\n",
    "  '''Get the next move for the bot\n",
    "  \n",
    "  Gets the next move for the bot with computations and\n",
    "  return value suitable for training only\n",
    "  (i.e. gradients are taken)\n",
    "  \n",
    "  If off_policy is not None, the off_policy move is\n",
    "  selected instead.\n",
    "  \n",
    "  Gradients with respect to the value-function applied\n",
    "  at the best move are accumlated and availabe to the caller\n",
    "  \n",
    "  Inputs\n",
    "  ------\n",
    "  \n",
    "  curr_state: binary tensor of shape (channels, rows, cols)\n",
    "              reprenting the game state\n",
    "              \n",
    "  off_policy: callable with signature\n",
    "              off_policy(num_available_moves: int) returning\n",
    "              either None or the index of the move desired.\n",
    "              If the return value is not None AND the bot \n",
    "              cannot otherwise win on that move, then that\n",
    "              move is made.\n",
    "              \n",
    "  profile   : verbosity, will determine whether to time and\n",
    "              display memory information\n",
    "              \n",
    "  Outputs\n",
    "  -------\n",
    "  game_over  : boolean. Whether the bot can (and does) win\n",
    "             on this move. The bot *always* plays a\n",
    "             win-in-one move when it is available, regardless\n",
    "             of the off-policy argument.\n",
    "             \n",
    "  off_policy : boolean. Whether an off-policy move was made.\n",
    "             OR: value is None if game is over\n",
    "  \n",
    "  new_state  : a binary tensor of same shape as curr_input\n",
    "             representing the new state of the game after\n",
    "             the bot moves AND the board is flipped around\n",
    "             to present it from opponents view. OR: value\n",
    "             is None if game is over.\n",
    "               \n",
    "  value      : value of the value-function applied to new_state.\n",
    "             OR: value is None if the game is over.\n",
    "  '''\n",
    "  # Compute the placements\n",
    "  placements = get_placements(curr_state)\n",
    "\n",
    "  # Compute the jumps\n",
    "  jumps = get_jumps(curr_state, MAX_JUMPS)\n",
    "\n",
    "  # Deal with special cases/win condition for the jump\n",
    "  \n",
    "  # No jumps to worry about\n",
    "  if len(jumps) == 0:\n",
    "    moves = placements\n",
    "  \n",
    "  # Win condition\n",
    "  elif (\n",
    "    len(jumps) == 1 and\n",
    "    jumps[0][CHAIN][END_LOC][COL] in [config.cols, config.cols-1]):\n",
    "    return True, None, None, None # The game is over!\n",
    "  \n",
    "  # Regular jump evaluation\n",
    "  else:\n",
    "    # Retain only the final state\n",
    "    jumps = [jump_data[0] for jump_data in jumps]\n",
    "    jumps = torch.tensor(jumps, dtype = torch.bool)\n",
    "    moves = torch.cat([placements, jumps])\n",
    "    \n",
    "  # Turn the board around to represent the opponent's view\n",
    "  moves = torch.flip(moves, [-1])\n",
    "  \n",
    "  # Either make an off policy move, or evaluate the value-function\n",
    "  #  to determine the policy\n",
    "  if off_policy(moves.shape[0]) is not None:\n",
    "    move     = moves[move].unsqueeze()\n",
    "    score, _ = model(move)\n",
    "    return False, True, move, score\n",
    "  \n",
    "  # Batch the moves\n",
    "  batches = torch.split(moves, BATCH_SIZE)\n",
    "  \n",
    "  # We only need to differentiate the best score\n",
    "  #  track which one that is.\n",
    "  best_score = None\n",
    "  best_index = None\n",
    "  curr_index = 0\n",
    "\n",
    "  if profile:\n",
    "    timer = Timer()\n",
    "    print(f'Starting forward pass')\n",
    "    print_memory_usage()\n",
    "    batch_num = 0\n",
    "    \n",
    "  for batch in batches:\n",
    "\n",
    "    \n",
    "    # Run the model\n",
    "    score, index = model(batch)\n",
    "\n",
    "    # Update running tally of best score\n",
    "    #  Old best scores should have their graphs\n",
    "    #  destroyed\n",
    "\n",
    "    if best_score is None or score > best_score:\n",
    "      best_score = score\n",
    "      best_index = curr_index + index\n",
    "\n",
    "    # Keep track of how many indices we've traversed to \n",
    "    #  get best_index correct\n",
    "    curr_index += batch.shape[0]\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    if profile:\n",
    "      print('\\n')\n",
    "      batch_num += 1\n",
    "      print(f'Completed batch{batch_num} after a total of {timer}')\n",
    "      if profile > 1:\n",
    "        print_memory_usage()\n",
    "\n",
    "  if profile > 1:\n",
    "    print_max_memory_usage()\n",
    "  # Return\n",
    "  return False, False, moves[best_index], best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Off-policy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy:\n",
    "  \n",
    "  def __init__(self, epsilon):\n",
    "    self.epsilon = epsilon\n",
    "  \n",
    "  def __call__(self, num_options):\n",
    "    if np.random.random() < epsilon:\n",
    "      return np.random.randint(0, num_options)\n",
    "    else:\n",
    "      return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "colab_type": "code",
    "id": "-Etz9nnawU0i",
    "outputId": "646b3347-ee2f-4cc5-a2d8-6bc1680dc8d8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "get_next_move_training(initial_state, profile = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EBpw7Ul8wUzs"
   },
   "outputs": [],
   "source": [
    "get_placements = create_placement_getter(device)\n",
    "epsilon_greedy = epsilon_greedy(0.1)\n",
    "\n",
    "model = TDConway(config).to(device)\n",
    "optimizer = AlternatingTDLambda(model.parameters(), 0.01, 0.9)\n",
    "\n",
    "initial_state = create_state('H10').to(device)\n",
    "visualize_state(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loop(optimizer, 2, off_policy = epsilon_greedy)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "model-training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

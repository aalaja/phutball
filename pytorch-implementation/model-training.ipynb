{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "model-training.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "PyTorch",
      "language": "python",
      "name": "torch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOBf9FoVO5gY",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ypbH-_VO5gZ",
        "colab_type": "text"
      },
      "source": [
        "## Create Filesystem\n",
        "This notebook is primarily meant to be executed in Colab as a computational backend. If you want to run on your own hardware with data, you need to set `data_dir` and `ALLOW_IO`\n",
        "\n",
        "This notebook viewable directly on Colab from [https://colab.research.google.com/github/rcharan/phutball/blob/rl/pytorch-implementation/model-training.ipynb](https://colab.research.google.com/github/rcharan/phutball/blob/rl/pytorch-implementation/model-training.ipynb) (it is a mirror of github). But if it has moved branches or you are looking at a past commit, look at the [Google instructions](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb) on where to find this file.\n",
        "\n",
        "The workflow is:\n",
        " - Data stored in (my personal/private) Google Drive\n",
        " - Utilities/library files (for importing) on github, edited on local hardware and pushed to github.\n",
        " - Notebook hosted on github, edited both in Colab or locally (depending on the relative value of having a GPU attached versus being able to use regular Jupyter keyboard shortcuts/a superior interface)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RY-nqnzUzB5k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "5f77094c-f3b1-4e4b-d5c4-7d0cce6279f9"
      },
      "source": [
        "# Attempt Colab setup if on Colab\n",
        "try:\n",
        "  import google.colab\n",
        "except:\n",
        "  ALLOW_IO = False\n",
        "else:\n",
        "  # Mount Google Drive at data_dir\n",
        "  #  (for data)\n",
        "  from google.colab import drive\n",
        "  from os.path import join\n",
        "  ROOT = '/content/drive'\n",
        "  DATA = 'My Drive/phutball'\n",
        "  drive.mount(ROOT)\n",
        "  ALLOW_IO = True\n",
        "  data_dir = join(ROOT, DATA)\n",
        "  !mkdir \"{data_dir}\"     # in case we haven't created it already   \n",
        "\n",
        "  # Pull in code from github\n",
        "  %cd /content\n",
        "  github_repo = 'https://github.com/rcharan/phutball'\n",
        "  !git clone -b rl {github_repo}\n",
        "  %cd /content/phutball\n",
        "  \n",
        "  # Point python to code base\n",
        "  import sys\n",
        "  sys.path.append('/content/phutball/pytorch-implementation')\n",
        "\n",
        "  # Updater for library functions changed on local hardware and pushed to github\n",
        "  #  (circuitous, I know)\n",
        "  def update_repo():\n",
        "    !git pull"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "mkdir: cannot create directory ‘/content/drive/My Drive/phutball’: File exists\n",
            "/content\n",
            "Cloning into 'phutball'...\n",
            "remote: Enumerating objects: 35, done.\u001b[K\n",
            "remote: Counting objects: 100% (35/35), done.\u001b[K\n",
            "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
            "remote: Total 1417 (delta 17), reused 24 (delta 12), pack-reused 1382\u001b[K\n",
            "Receiving objects: 100% (1417/1417), 6.33 MiB | 25.42 MiB/s, done.\n",
            "Resolving deltas: 100% (845/845), done.\n",
            "/content/phutball\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF1C55w-O5gg",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jbaorYhzwUze",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import os\n",
        "import gc\n",
        "import numpy as np\n",
        "\n",
        "# Codebase\n",
        "from lib.model_v1          import TDConway\n",
        "from lib.moves             import create_placement_getter, get_jumps\n",
        "from lib.utilities         import config, lfilter\n",
        "from lib.testing_utilities import create_state, visualize_state, boards\n",
        "from lib.timer             import Timer\n",
        "from lib.moves             import END_LOC, COL, CHAIN\n",
        "\n",
        "# Graphics for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "%matplotlib inline\n",
        "plt.ioff()\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "from torch.optim import Optimizer\n",
        "\n",
        "# Tensorflow solely for the Progress Bar (totally worth it)\n",
        "from tensorflow.keras.utils import Progbar as ProgressBar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOk4kKeOO5gm",
        "colab_type": "text"
      },
      "source": [
        "## Device Management Utilities\n",
        "Setup for GPU, CPU, or (not working well/fully implemented) TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7ETHYgbO5gm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "use_tpu = False\n",
        "\n",
        "if use_tpu:\n",
        "  # Install PyTorch/XLA\n",
        "  VERSION = \"nightly\" #[\"20200220\",\"nightly\", \"xrt==1.15.0\"]\n",
        "  !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "  !python pytorch-xla-env-setup.py --version $VERSION\n",
        "  import torch_xla\n",
        "  import torch_xla.core.xla_model as xm\n",
        "  \n",
        "  # Set the device\n",
        "  device = xm.xla_device()\n",
        "  \n",
        "  # Memory inspection\n",
        "  def print_memory_usage():\n",
        "    print('TPU memory inspection not implemented')\n",
        "  def print_max_memory_usage():\n",
        "    print('TPU memory inspection not implemented')\n",
        "  def garbage_collect():\n",
        "    gc.collect() # No TPU specific implementation yet\n",
        "    \n",
        "elif torch.cuda.is_available():\n",
        "  # Set the device\n",
        "  device = torch.device('cuda')\n",
        "  \n",
        "  # Echo GPU info\n",
        "  gpu_info = !nvidia-smi\n",
        "  gpu_info = '\\n'.join(gpu_info)\n",
        "  print(gpu_info)\n",
        "  \n",
        "  # Memory inspection and management\n",
        "  from lib.memory import (\n",
        "    print_memory_usage_cuda     as print_memory_usage,\n",
        "    print_max_memory_usage_cuda as print_max_memory_usage,\n",
        "    garbage_collect_cuda        as garbage_collect\n",
        "  )\n",
        "\n",
        "else:\n",
        "  # Set the device to CPU\n",
        "  device = torch.device('cpu')\n",
        "  \n",
        "  # Echo RAM info\n",
        "  from psutil import virtual_memory\n",
        "  from lib.memory import format_bytes\n",
        "  ram = virtual_memory().total\n",
        "  print(format_bytes(ram), 'available memory on CPU-based runtime')\n",
        "  \n",
        "  # Memory inspection and management\n",
        "  from lib.memory import (\n",
        "    print_memory_usage, \n",
        "    print_max_memory_usage,\n",
        "    garbage_collect\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cE3ErAdVO5gt",
        "colab_type": "text"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Kl9_Q1WNwUz8",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "MAX_JUMPS = 300"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmRqSIoWO5gy",
        "colab_type": "text"
      },
      "source": [
        "# PyTorch Tools\n",
        "These tools will be moved to .py files in the lib folder when stable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZnQLuYPO5gy",
        "colab_type": "text"
      },
      "source": [
        "## Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rkzXSlsbwU0C",
        "colab": {}
      },
      "source": [
        "class AlternatingTDLambda(Optimizer):\n",
        "  '''Implements tracing and updates for the TD(λ) algorithm.\n",
        "  \n",
        "  For details see Sutton and Barto, Reinforcement Learning 2ed,\n",
        "  Chaper 12 Section 2.\n",
        "  \n",
        "  Modifications:\n",
        "    (1) the algorithm is modified to compute traces\n",
        "        one step early (when the graph is available). Hence, trace must be\n",
        "        initialized with update_trace before calling step.\n",
        "        \n",
        "    (2) Because the board switches sides each turn, eligibility trace updates\n",
        "        must be of the form z_t+1 <-  -λz_t + 𝝯v\n",
        "  \n",
        "  You may find the following greeks a helpful reference:\n",
        "  alpha : Learning Rate\n",
        "  lambda: Exponential Decay parameter for the eligibility trace\n",
        "          (which is essentially momentum but with different\n",
        "           interpertation and is occasionally zeroed out).\n",
        "  delta : Temporal difference (TD) i.e. the difference between the estimated\n",
        "          value of a step and the realized value upon the best move (based\n",
        "          on further estimation of course).\n",
        "  '''\n",
        "  def __init__(self, parameters, alpha, lamda):\n",
        "    self.alpha = alpha\n",
        "    self.lamda = lamda # Note alternate spelling (I didn't make it up!)\n",
        "    defaults   = dict(alpha = alpha, lamda = lamda)\n",
        "\n",
        "    super(AlternatingTDLambda, self).__init__(parameters, defaults)\n",
        "    \n",
        "  @torch.no_grad()\n",
        "  def step(self, delta, update_trace = True):\n",
        "    '''Performs a single optimization step updating the trace *afterwards*\n",
        "    \n",
        "    update_trace must be called before first step to initialize the trace.\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    \n",
        "    delta: Difference between estimated value of move at time t and realized\n",
        "           value after moving and going to time t+1\n",
        "    '''\n",
        "    \n",
        "    for group in self.param_groups:\n",
        "      alpha = group['alpha']\n",
        "      \n",
        "      for p in group['params']:\n",
        "        if p.grad is None:\n",
        "          continue\n",
        "        \n",
        "        state = self.state[p]\n",
        "        \n",
        "        if len(state) == 0 or 'trace' not in state:\n",
        "          raise RuntimeError('Traces must be initialized before calling step')\n",
        "          \n",
        "        trace = state['trace']\n",
        "        \n",
        "        p.add_(alpha * delta * trace) # Note gradient *ascent* in reinforcement learning\n",
        "    \n",
        "    if update_trace:\n",
        "      self._update_trace()\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def _update_trace(self):\n",
        "    '''Updates the trace based on the gradients.\n",
        "    \n",
        "    This also is the only way to initialize the traces.\n",
        "    It must be called after evaluating the starting position\n",
        "    and backprop at the beginning of the game, usually from\n",
        "    the optimizers restart method.\n",
        "    '''\n",
        "    for group in self.param_groups:\n",
        "      lamda = group['lamda']\n",
        "      \n",
        "      for p in group['params']:\n",
        "        if p.grad is None:\n",
        "          continue\n",
        "        \n",
        "        state = self.state[p]\n",
        "        \n",
        "        # Initialize to zero if necessary\n",
        "        if len(state) == 0 or 'trace' not in state:\n",
        "          state['trace'] = torch.zeros_like(p)\n",
        "          \n",
        "        trace = state['trace']\n",
        "        trace.mul_(-lamda).add_(p.grad)\n",
        "      \n",
        "  @torch.no_grad()\n",
        "  def _zero_trace(self):\n",
        "    for group in self.param_groups:\n",
        "      for p in group['params']:\n",
        "        state = self.state['p']\n",
        "        if 'trace' not in state:\n",
        "          continue\n",
        "          \n",
        "        trace = state['trace']\n",
        "        trace.zero_()\n",
        "    \n",
        "  def restart(self, score):\n",
        "    '''Call to restart the trace\n",
        "    \n",
        "    Returns score.item() as a convenience\n",
        "    '''\n",
        "    self._zero_trace()\n",
        "    score.backward()\n",
        "    self._update_trace()\n",
        "    self.zero_grad()\n",
        "    return score.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-RCMAEuO5g2",
        "colab_type": "text"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p1CAfi-4wU0H",
        "colab": {}
      },
      "source": [
        "def training_loop(optimizer, num_games, off_policy = lambda _ : None):\n",
        "  \n",
        "  initial_state = create_state('H10').to(device)\n",
        "  for i in range(num_games):\n",
        "    print(f'\\nPlaying game {i+1} of {num_games}:')\n",
        "    game_loop(initial_state, model, optimizer, off_policy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "62GkFibrwU0Q",
        "colab": {}
      },
      "source": [
        "def game_loop(initial_state, model, optimizer, off_policy):\n",
        "  '''Training loop that plays one game'''\n",
        "  # Just in case\n",
        "  optimizer.zero_grad()\n",
        "  \n",
        "  # Initialization\n",
        "  state    = initial_state\n",
        "  score, _ = model(state.unsqueeze(0))\n",
        "  v_t      = optimizer.restart(score)\n",
        "  \n",
        "  # Progress Bar\n",
        "  bar      = ProgressBar(284)\n",
        "  move_num = 1\n",
        "  \n",
        "  while True:\n",
        "    # Determine the next move\n",
        "    game_over, moved_off_policy, new_state, score = \\\n",
        "      get_next_move_training(state, off_policy = off_policy)\n",
        "    \n",
        "    if game_over:      \n",
        "      delta = 1 - v_t\n",
        "      optimizer.step(delta, update_trace = False)\n",
        "      \n",
        "      # Terminate the progress bar\n",
        "      bar.target = move_num\n",
        "      bar.update(move_num)\n",
        "\n",
        "      break\n",
        "    \n",
        "    elif moved_off_policy:\n",
        "      # Equivalent to starting a new game\n",
        "      v_t = optimizer.restart(score)\n",
        "      \n",
        "    else:\n",
        "      score.backward()\n",
        "      delta = (1 - score) - v_t\n",
        "      optimizer.step(delta)\n",
        "      optimizer.zero_grad()\n",
        "      \n",
        "      v_t   = score.item()\n",
        "      state = new_state\n",
        "      \n",
        "    # Progress bar\n",
        "    move_num += 1\n",
        "    if move_num >= bar.target * 0.9:\n",
        "      bar.target += bar.target // 10 + 1\n",
        "    \n",
        "    bar.update(move_num)\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2_Kl9lNTwU0T",
        "colab": {}
      },
      "source": [
        "def get_next_move_training(curr_state, off_policy = lambda _ : None, profile = 0):\n",
        "  '''Get the next move for the bot\n",
        "  \n",
        "  Gets the next move for the bot with computations and\n",
        "  return value suitable for training only\n",
        "  (i.e. gradients are taken)\n",
        "  \n",
        "  If off_policy is not None, the off_policy move is\n",
        "  selected instead.\n",
        "  \n",
        "  Gradients with respect to the value-function applied\n",
        "  at the best move are accumlated and availabe to the caller\n",
        "  \n",
        "  Inputs\n",
        "  ------\n",
        "  \n",
        "  curr_state: binary tensor of shape (channels, rows, cols)\n",
        "              reprenting the game state\n",
        "              \n",
        "  off_policy: callable with signature\n",
        "              off_policy(num_available_moves: int) returning\n",
        "              either None or the index of the move desired.\n",
        "              If the return value is not None AND the bot \n",
        "              cannot otherwise win on that move, then that\n",
        "              move is made.\n",
        "              \n",
        "  profile   : verbosity, will determine whether to time and\n",
        "              display memory information\n",
        "              \n",
        "  Outputs\n",
        "  -------\n",
        "  game_over  : boolean. Whether the bot can (and does) win\n",
        "             on this move. The bot *always* plays a\n",
        "             win-in-one move when it is available, regardless\n",
        "             of the off-policy argument.\n",
        "             \n",
        "  off_policy : boolean. Whether an off-policy move was made.\n",
        "             OR: value is None if game is over\n",
        "  \n",
        "  new_state  : a binary tensor of same shape as curr_input\n",
        "             representing the new state of the game after\n",
        "             the bot moves AND the board is flipped around\n",
        "             to present it from opponents view. OR: value\n",
        "             is None if game is over.\n",
        "               \n",
        "  value      : value of the value-function applied to new_state.\n",
        "             OR: value is None if the game is over.\n",
        "  '''\n",
        "  # Compute the placements\n",
        "  placements = get_placements(curr_state)\n",
        "\n",
        "  # Compute the jumps\n",
        "  jumps = get_jumps(curr_state, MAX_JUMPS)\n",
        "\n",
        "  # Deal with special cases/win condition for the jump\n",
        "  \n",
        "  # No jumps to worry about\n",
        "  if len(jumps) == 0:\n",
        "    moves = placements\n",
        "  \n",
        "  # Win condition\n",
        "  elif (\n",
        "    len(jumps) == 1 and\n",
        "    jumps[0][CHAIN][END_LOC][COL] in [config.cols, config.cols-1]):\n",
        "    return True, None, None, None # The game is over!\n",
        "  \n",
        "  # Regular jump evaluation\n",
        "  else:\n",
        "    # Retain only the final state\n",
        "    jumps = [jump_data[0] for jump_data in jumps]\n",
        "    jumps = torch.tensor(jumps, dtype = torch.bool, device = device)\n",
        "    moves = torch.cat([placements, jumps])\n",
        "    \n",
        "  # Turn the board around to represent the opponent's view\n",
        "  moves = torch.flip(moves, [-1])\n",
        "  \n",
        "  # Either make an off policy move, or evaluate the value-function\n",
        "  #  to determine the policy\n",
        "  off_policy_move = off_policy(moves.shape[0])\n",
        "  if off_policy_move is not None:\n",
        "    move     = moves[off_policy_move].unsqueeze(0)\n",
        "    score, _ = model(move)\n",
        "    return False, True, move, score\n",
        "  \n",
        "  # Batch the moves\n",
        "  batches = torch.split(moves, BATCH_SIZE)\n",
        "  \n",
        "  # We only need to differentiate the best score\n",
        "  #  track which one that is.\n",
        "  best_score = None\n",
        "  best_index = None\n",
        "  curr_index = 0\n",
        "\n",
        "  if profile:\n",
        "    timer = Timer()\n",
        "    print(f'Starting forward pass')\n",
        "    print_memory_usage()\n",
        "    batch_num = 0\n",
        "    \n",
        "  for batch in batches:\n",
        "\n",
        "    \n",
        "    # Run the model\n",
        "    score, index = model(batch)\n",
        "\n",
        "    # Update running tally of best score\n",
        "    #  Old best scores should have their graphs\n",
        "    #  destroyed\n",
        "\n",
        "    if best_score is None or score > best_score:\n",
        "      best_score = score\n",
        "      best_index = curr_index + index\n",
        "\n",
        "    # Keep track of how many indices we've traversed to \n",
        "    #  get best_index correct\n",
        "    curr_index += batch.shape[0]\n",
        "    \n",
        "    \n",
        "    if profile:\n",
        "      print('')\n",
        "      batch_num += 1\n",
        "      print(f'Completed batch {batch_num} after a total of {timer}')\n",
        "      if profile > 1:\n",
        "        print_memory_usage()\n",
        "\n",
        "  if profile > 1:\n",
        "    print_max_memory_usage()\n",
        "  \n",
        "  # Return\n",
        "  return False, False, moves[best_index], best_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4-LF0-RO5hA",
        "colab_type": "text"
      },
      "source": [
        "# Off-policy policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n316gx2nO5hA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EpsilonGreedy:\n",
        "  \n",
        "  def __init__(self, epsilon):\n",
        "    self.epsilon = epsilon\n",
        "  \n",
        "  def __call__(self, num_options):\n",
        "    if np.random.random() < self.epsilon:\n",
        "      return np.random.randint(0, num_options)\n",
        "    else:\n",
        "      return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxB31wNMO5hG",
        "colab_type": "text"
      },
      "source": [
        "# Execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDiYyY9KPXIE",
        "colab_type": "text"
      },
      "source": [
        "## Instantiate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EBpw7Ul8wUzs",
        "colab": {}
      },
      "source": [
        "get_placements = create_placement_getter(device)\n",
        "epsilon_greedy = EpsilonGreedy(0.1)\n",
        "\n",
        "model = TDConway(config).to(device)\n",
        "optimizer = AlternatingTDLambda(model.parameters(), 0.01, 0.9)\n",
        "\n",
        "initial_state = create_state('H10').to(device)\n",
        "visualize_state(initial_state)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlOE_I84O5hD",
        "colab_type": "text"
      },
      "source": [
        "## Profile"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-Etz9nnawU0i",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "# # Blazing fast on a P100!\n",
        "# %%prun\n",
        "# get_next_move_training(initial_state);\n",
        "# _ = _"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B20z3tcFO5hK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Decently fast. 84ms/move on P100\n",
        "# %%prun\n",
        "# training_loop(optimizer, 1, off_policy = epsilon_greedy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqLsixqoP3kV",
        "colab_type": "text"
      },
      "source": [
        "## Run!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpNxzfZaQyTp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_loop(optimizer, 200, off_policy = epsilon_greedy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHORM9o9TCgY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save(fname, model, optimizer):\n",
        "  state_dict = {\n",
        "      'model' : model.state_dict(),\n",
        "      'optim' : optimizer.state_dict()\n",
        "  }\n",
        "  torch.save(state_dict, f'{data_dir}/{fname}.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pFxrcCKURwK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "version = '0.1.1'\n",
        "game_num = 376\n",
        "save(f'v{version}-{game_num}', model, optimizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBcZ0mdydqwI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
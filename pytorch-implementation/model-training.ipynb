{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "PyTorch",
      "language": "python",
      "name": "torch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "model-training.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RY-nqnzUzB5k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Open this notebook in Colab (if you want to train)\n",
        "# at https://colab.research.google.com/github/rcharan/phutball/blob/rl/pytorch-implementation/model-training.ipynb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNflNJ9jxDhj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c1e21740-a268-4a1d-adbc-002e06666614"
      },
      "source": [
        "# Mount Google Drive at data_dir\n",
        "#  (for data)\n",
        "from google.colab import drive\n",
        "from os.path import join\n",
        "ROOT = '/content/drive'\n",
        "DATA = 'My Drive/phutball'\n",
        "drive.mount(ROOT)\n",
        "data_dir = join(ROOT, PROJ)\n",
        "!mkdir \"{data_dir}\"     # in case we haven't created it already   "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "mkdir: cannot create directory ‘/content/drive/My Drive/phutball’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRiJoZ6mwrgY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "f117018d-69e5-4e24-fc1b-11327b00598c"
      },
      "source": [
        "# Pull in code from github\n",
        "github_repo = 'https://github.com/rcharan/phutball'\n",
        "!git clone {github_repo}\n",
        "%cd phutball\n",
        "!git checkout rl"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'phutball'...\n",
            "remote: Enumerating objects: 1361, done.\u001b[K\n",
            "remote: Counting objects: 100% (1361/1361), done.\u001b[K\n",
            "remote: Compressing objects: 100% (744/744), done.\u001b[K\n",
            "remote: Total 1361 (delta 801), reused 1131 (delta 578), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (1361/1361), 6.29 MiB | 17.78 MiB/s, done.\n",
            "Resolving deltas: 100% (801/801), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbaorYhzwUze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Lh-L2-pwUzk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from lib.model_v1 import TDConway\n",
        "from lib.moves import create_placement_getter, get_jumps\n",
        "from lib.utilities import config\n",
        "\n",
        "from lib.testing_utilities import create_state, visualize_state, boards\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.ioff()\n",
        "\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qxe7mdQswUzn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import psutil\n",
        "\n",
        "\n",
        "import gc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBpw7Ul8wUzs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cpu')\n",
        "get_placements = create_placement_getter(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKTjg7fcwUzv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = TDConway(config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bk_MuwokwUzz",
        "colab_type": "code",
        "colab": {},
        "outputId": "6b5f89b5-b475-4d26-9a83-112de9d6800e"
      },
      "source": [
        "initial_state = create_state('H10')\n",
        "visualize_state(initial_state)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWwAAAD4CAYAAADIH9xYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAVOUlEQVR4nO3deZBldXnG8e8rgziLIjqAKERwAReSICOuyDZqEAkBo6jRCkYTSysExWhEtBRjmbig0aglpeISN1xQQRIRIiCmSlQGh01UQhhlFAdxHxhZhjd/3DPMped29zmnz+k5v/b7qbo1t+/c88zTPbffPn3uub8bmYkkafjutrULSJLqcWBLUiEc2JJUCAe2JBXCgS1JhVjUZ/jy5ctz9913n/E+N910E0uXLp3Tv9NFxkLs0lXOkLp0lWOXfnOG1KWrnPnssmrVqhszc8ct/iIze7usWLEiZ3P++efPep/5yOgqZ0hdusoZUpeucuzSb86QunSVM59dgItzwkz1kIgkFcKBLUmFaDSwI+KoiMiIeFhfhSRJkzXdw34u8D/Ac3roIkmaQe2BHRHLgCcCL8KBLUnzrske9pHA2Zn5Q+CXEbFvT50kSRNE1lytLyL+E3hXZp4bEccBu2Xmqybc78XAiwF23nnnFaeddtqMuevXr2fZsmWNi3edsRC7dJUzpC5d5dil35whdekqZz67HHzwwasy89Fb/MWkc/2mXoD7AhuAHwFrgOuAH1MN/Okunoe9MHKG1KWrHLv0mzOkLl3llHQe9jOB/8jMB2bm7pm5G3AtsH/N7SVJc1R3YD8X+OKU204H/qrbOpKk6dRaSyQzD5pw27933kaSNC1f6ShJhXBgS1IhHNiSVAgHtiQVwoEtSYVwYEtSIZos/rQxIlZHxKURcUlEPKHPYpKku2ryno4bMnMfgIj4M+BfgQN7aSVJ2kLbQyL3An7VZRFJ0syarNa3EbgcuAewC3BIZq6acD9X6xtIl65yhtSlqxy79JszpC5d5RSzWl811NePXX88cCWu1tdLxtByhtSlqxy79JszpC5d5ZS0Wt/UIf9NYDmwY5vtJUnNtRrY1ZvwbgP8ots6kqTpNDlLZHFErK6uB3BMZm7soZMkaYLaAzszt+mziCRpZr7SUZIK4cCWpEI4sCWpEA5sSSqEA1uSCtFktb77RcRpEXFNRHwvIv4rIvbss5wkabNaAzsiAvgicEFmPjgzHwGcCOzcZzlJ0mZ1z8M+GLgtM0/ZdENmrp7h/pKkjtU9JLI3sMXKfJKk+VNredWIOA7YIzOPr3Ffl1cdSJeucobUpascu/SbM6QuXeUUs7wqsBK4sM590+VV55wxtJwhdekqxy795gypS1c5JS2veh6wXUT83aYbImK/iPAtwiRpntQa2NXEPwp4SnVa35XAScBPe+wmSRrTZLW+nwJH99hFkjQDX+koSYVwYEtSIRzYklQIB7YkFcKBLUmFcGBLUiFqndYXERuBy4FtgduBjwHvysw7euwmSRpT9zzsDZm5D0BE7AR8CtgeeENfxSRJd9X4kEhm3sBocadjq3WyJUnzoO5qfeszc9mU234FPCwz10253dX6BtKlq5whdekqxy795gypS1c5Ja3Wt37Cbb8Gdp5pO1frWxg5Q+rSVY5d+s0ZUpeuckpare8uIuJBwEbghjbbS5KaazywI2JH4BTgvdVPAknSPKh7lsjiiFjN5tP6Pg68s7dWkqQt1BrYmblN30UkSTPzlY6SVAgHtiQVwoEtSYVwYEtSIRzYklSI2m/CC3dZtW+T0zLzLd1WkiRN0mhgM7ZqnyRpfnlIRJIK0XRgL46I1WOXZ/fSSpK0hVrLq9555wnLrE64j8urDqRLVzlD6tJVjl36zRlSl65yilleNWdYZnWmi8urLoycIXXpKscu/eYMqUtXOcUurypJmn9NzxLZtGrfJmdn5gldFpIkTdZoYKer9knSVuMhEUkqhANbkgrhwJakQjiwJakQDmxJKoQDW5IKMdflVY/MzDWdNpIkTeTyqpJUCA+JSFIhmq7WN35I5NrMPGrCfVytbyBdusoZUpeucuzSb86QunSV42p9NVeumo+MrnKG1KWrnCF16SrHLv3mDKlLVzmu1idJqs2BLUmFcGBLUiEaDeyc5e3BJEn9cQ9bkgrhwJakQjiwJakQDmxJKoQDW5IKUXtgR8TOEfGpiPi/iFgVEd+MiC1emi5J6ketgR0RAXwJuDAzH5SZK4DnALv2WU6StFnd5VUPAW7NzFM23ZCZPwLe00srSdIW6h4SeSRwSZ9FJEkzq7W8akQcB+yRmcdXH78P2J/RXvd+U+7r8qoD6dJVzpC6dJVjl35zhtSlq5xillcFVgJfn3LbcmDNTNu5vOrCyBlSl65y7NJvzpC6dJVT0vKq5wH3iIiXjt22pOa2kqQO1BrY1cQ/EjgwIq6NiG8DHwNe3Wc5SdJmtd+ENzOvZ3QqnyRpK/CVjpJUCAe2JBXCgS1JhXBgS1IhHNj6w3HllfCa18BjHgPf+x4ceSScfjps2LC1m0m11D5LRCrWLbfAMcfAmWfC7bfDbbfBs58NZ5wB550H220HZ50Fj33s1m4qzajuan3rp3z8goh4bz+VpI4dffRoWG/YMBrW4373O7jxRli5Eq6+euv0k2rykIgWtksugXPPnf2wx+9/DyecMD+dpJYc2FrYPve50TCezcaNo73wGouhSVtL3dX6NgKXj910H+DMzDx2wn1drW8gXbrKGVKXxjlr18K6dVtm7Lory9au3fL+K1b016XHjKHlDKlLVzklrda3fsrHLwDeO9t2rta3MHKG1KVxzjnnZC5dmjnad77zcv7JJ9/1tojM/ffvt0uPGUPLGVKXrnJKWq1PKtOTnwx77QXbbjvz/RYvhpNPnp9OUksObC1sEXD22fDIR8KkX0O32w6WLIFTT/W0Pg2eA1sL3447wne+Ax/5CBx6KOywA9ztbvCQh8CJJ45eRPMcF6LU8NV64UxmLpvy8UeBj/bQR+rHokXwzGeOLgAXXOB51yqOe9iSVAgHtiQVwoEtSYVwYEtSIRzYklSI1gN76gp+kqR+uYctSYVwYEtSIRzYklSIWsurTtwwYv3UV0BWt7u86kC6dJUzpC5d5dil35whdekqp5jlVSddmLLk6qSLy6sujJwhdekqxy795gypS1c5Lq8qSaqt1cCOiEXALR13kSTNoO0e9iOBa7osIkmaWeOBHREvAT4NvK77OpKk6dRaD3tcZp4CnNJDF0nSDHzSUZIK4cCWpEI4sCWpEA5sSSpE7YE9vpxqRBwWEVdHxB/1U0uSNFXjs0QiYiXwHuCpmfnj7itJkiZpNLAj4knAB4HDMtMXzkjSPKq9Wl9E3Ab8DjgoMy+b4X6u1jeQLl3lDKlLVzl26TdnSF26yilqtT7gZuAs4N11t3G1voWRM6QuXeXYpd+cIXXpKqe01fruAI4G9ouIExtsJ0nqQKNj2Jl5c0QcDnwjItZl5qk99ZIkTdFmLZFfRsShwIURcWNmntFDL0nSFLUHdo69HVhmXgfs0UsjSdJEvtJRkgrhwJakQjiwJakQDmxJKoQDW5IK0Whgj6/YJ0maX+5hS1IhHNiSVAgHtiQVovbyqjA6hj3+isdp7uPyqgPp0lXOkLp0lWOXfnOG1KWrnKKWV60G+/om93d51YWRM6QuXeXYpd+cIXXpKqe05VUlSVtR04G9JCLWjl1e0UsrSdIWmq6H7R65JG0lDmBJKoQDW5IK4cCWpEI4sCWpEA5sSSqEA1uSCjHrwI6IjIiPj328KCJ+HhFn9VtNkjSuzh72TcDeEbG4+vgpwE/6qyRJmqTuIZGvAE+vrj8X+HQ/dSRJ05l1tb7qXWaeALweeD5wEfBy4JWZefiE+7ta30C6dJUzpC5d5dil35whdekqp4jV+qhW6AMuBv4G+BfgIOCs2bZ1tb6FkTOkLl3l2KXfnCF16SpnCKv1NVlL5Ezg5GpY37fBdpKkDjQZ2B8GfpOZl0fEQT31kSRNo/bAzsy1wLt77CJJmsGsAzsnvCVYZl4AXNBDH0nSNHyloyQVwoEtSYVwYEtSIRzYklQIB7YkFaLWwK5W7HvH2MevjIiTemslSdpC3T3sW4BnRMTyPstIkqZXd2DfDnwAOL7HLpKkGTQ5hv0+4HkRsX1fZSRJ05t1eVUYLbGamcsi4p+B24ANwLLMPGnCfV1edSBdusoZUpeucuzSb86QunSVU8TyqnnXJVbvA6wB3gCcNNt2Lq+6MHKG1KWrHLv0mzOkLl3lDGF51Uan9WXmL4HPAi9qsp0kae7anIf9DsCzRSRpntVaXjXHVuzLzHXAkt4aSZIm8pWOklQIB7YkFcKBLUmFcGBLUiEc2JJUCAe2JBWi9sCOiF0j4oyIuDoiromId0fE3fssJ0narO562AF8AfhSZj4U2BNYBry5x26SpDF197APAX6fmR8ByMyNjJZafWFE+CIaSZoHdVfrOw7YIzOPn3L7d4FjMvOysdtcrW8gXbrKGVKXrnLs0m/OkLp0lVPSan0vA9454fbVwB9Pt52r9S2MnCF16SrHLv3mDKlLVzklrdZ3JXCXaR8R9wJ2A66pmSFJmoO6A/trwJKI+GuAiNiG0ap9H83Mm/sqJ0narNbArnbRjwKeFRFXAz8Efg+c2GM3SdKYWsurAmTmdcCf99hFkjQDX+koSYVwYEtSIRzYklQIB7YkFcKBLUmFqH2WCEBEbAQur7a7itHL0j0PW5LmQdM97A2ZuU9m7g3cCrykh06SpAnmckjkG8BDuioiSZpZq4EdEYuApzE6PCJJmge1lle9886bj2HDaA/7HzPz1in3cXnVgXTpKmdIXbrKsUu/OUPq0lVOMcur5ublVNc3ub/Lqy6MnCF16SrHLv3mDKlLVzklLa8qSdrKHNiSVIhGAzsz534AR5LUinvYklQIB7YkFaLRaX2NwyN+DvxolrstB26c4z/VRcZC7NJVzpC6dJVjl35zhtSlq5z57PLAzNxxi1snnToynxemOX1lvjMWYhc/J7v4OS2sz8lDIpJUCAe2JBViCAP7AwPJ6CpnSF26yhlSl65y7NJvzpC6dJWz1bv0+qSjJKk7Q9jDliTV4MCWpEJstYEdER+OiBsi4oo5ZOwWEedHxFURcWVEvKxFxj0i4tsRcWmV8ca2faq8bSLiuxFx1hwy1kTE5RGxOiIubplx74j4fER8v/r6PL5Fxl5Vh02X30bEy1vkHF99ba+IiE9HxD2aZlQ5L6syrmzSY9JjLSLuExHnRsTV1Z87tMx5VtXnjojYcjnMehlvr/6fLouIL0bEvVvmvKnKWB0R50TE/ZtmjP3dKyMiI2J5yy4nRcRPxh47h7XJqW7/h4j4QfV1fluLLp8Z67EmIla3/Jz2iYiLNn1fRsRjWub8aUR8s/oe/3JE3Gu2nDt1cV5hy3MRDwD2Ba6YQ8YuwL7V9XsCPwQe0TAjgGXV9W2BbwGPm0OnVwCfAs6aQ8YaYPkcv74fA/62un534N5zzNsG+BmjE/qbbPcA4FpgcfXxZ4EXtPj39wauAJYwek/R/wYe2vaxBrwNOKG6fgLw1pY5Dwf2Ai4AHt0y46nAour6W+fQ5V5j148DTmmaUd2+G/BVRi96m/VxOE2Xk4BXNvw/npRzcPV/vV318U5tPqexv38H8PqWXc4BnlZdPwy4oGXOd4ADq+svBN5U92u01fawM/NC4JdzzLg+My+prv+O0RsDP6BhRmbm+urDbatLq2diI2JX4OnAh9ps35XqJ/YBwKkAmXlrZv56jrErgWsyc7ZXrk6yCFhcvVPREuCnLTIeDlyUmTdn5u3A14Gj6mw4zWPtLxj9UKP688g2OZl5VWb+oE6PGTLOqT4ngIuAXVvm/Hbsw6XM8jie4Xvw34B/mm37GjmNTJPzUuAtmXlLdZ8b2naJiACOBj7dsksCm/aGt6fG43ianL2AC6vr5wJ/OVvOJgvmGHZE7A48itEectNtt6l+TboBODczG2dU3sXogX5Hy+03SeCciFhVvYNPUw8Cfg58pDo886GIWDrHTs+hxgN9qsz8CXAy8GPgeuA3mXlOi3//CuCAiLhvRCxhtIezW4ucTXbOzOurjtcDO80hq0svBL7SduOIeHNEXAc8D3h9i+2PAH6SmZe27TDm2OoQzYfrHHKaxp7AkyLiWxHx9YjYbw59ngSsy8yrW27/cuDt1df3ZOA1LXOuAI6orj+LBo/jBTGwI2IZcDrw8il7GbVk5sbM3IfRns1jImLvFh0OB27IzFVNt53giZm5L6P3zfz7iDig4faLGP0a9v7MfBRwE6Nf+1uJiLszeoB9rsW2OzDam90DuD+wNCKe3zQnM69idLjgXOBs4FLg9hk3KkxEvJbR5/TJthmZ+drM3K3KOLbhv78EeC0tBv0E7wceDOzD6Af1O1rmLAJ2AB4HvAr4bLWn3MZzabHTMealwPHV1/d4qt9gW3gho+/rVYwO5d46y/3vVPzAjohtGQ3rT2bmF+aSVR02uAA4tMXmTwSOiIg1wGnAIRHxiZY9flr9eQPwRWDWJzemWAusHftN4fOMBnhbTwMuycx1LbZ9MnBtZv48M28DvgA8oU2JzDw1M/fNzAMY/ZrZdk8JYF1E7AJQ/Tnjr9p9i4hjgMOB52V1cHOOPkWDX7UrD2b0g/XS6nG8K3BJRNyv6T+emeuqHaE7gA/S/DG8yVrgC9Why28z+u111idCp6oOxz0D+EzLHgDHMHr8wmjnpdXnlJnfz8ynZuYKRj9Arqm7bdEDu/pJeypwVWa+s2XGjpuelY+IxYwGzPeb5mTmazJz18zcndHhg/Mys/GeZEQsjYh7brrO6AmpRmfSZObPgOsiYq/qppXA95p2GTOXPZMfA4+LiCXV/9dKRs81NBYRO1V//hGjb7657C2dyegbkOrPM+aQNScRcSjwauCIzLx5DjkPHfvwCBo+jjPz8szcKTN3rx7Haxk9qf+zFl12GfvwKBo+hsd8CTikytyT0RPobVbMezLw/cxc27IHjI5ZH1hdP4SWOwxjj+O7Aa8DTqm9cd1nJ7u+MPpmux64jdED40UtMvZndLz3MmB1dTmsYcafAN+tMq6gxjPINTIPouVZIoyOP19aXa4EXtsyZx/g4urz+hKwQ8ucJcAvgO3n8PV4I6PhcQXwcapn/FvkfIPRD55LgZVzeawB9wW+xuib7mvAfVrmHFVdvwVYB3y1Rcb/AteNPYZnPLtjhpzTq6/xZcCXgQc0zZjy92uod5bIpC4fBy6vupwJ7NIy5+7AJ6rP6xLgkDafE/BR4CVzfMzsD6yqHn/fAla0zHkZozPafgi8heoV53UuvjRdkgpR9CERSfpD4sCWpEI4sCWpEA5sSSqEA1uSCuHAlqRCOLAlqRD/D2ppBXjSMFvxAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DwZFYQAwUz3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "curr_state = initial_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kl9_Q1WNwUz8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "MAX_JUMPS = 300\n",
        "from lib.moves import END_LOC, COL, CHAIN"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yF7BrTwZwUz_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.optim import Optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkzXSlsbwU0C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AlternatingTDLambda(Optimizer):\n",
        "  '''Implements tracing and updates for the TD(λ) algorithm.\n",
        "  \n",
        "  For details see Sutton and Barto, Reinforcement Learning 2ed,\n",
        "  Chaper 12 Section 2.\n",
        "  \n",
        "  Modifications:\n",
        "    (1) the algorithm is modified to compute traces\n",
        "        one step early (when the graph is available). Hence, trace must be\n",
        "        initialized with update_trace before calling step.\n",
        "        \n",
        "    (2) Because the board switches sides each turn, eligibility trace updates\n",
        "        must be of the form z_t+1 <-  -λz_t + 𝝯v\n",
        "  \n",
        "  You may find the following greeks a helpful reference:\n",
        "  alpha : Learning Rate\n",
        "  lambda: Exponential Decay parameter for the eligibility trace\n",
        "          (which is essentially momentum but with different\n",
        "           interpertation and is occasionally zeroed out).\n",
        "  delta : Temporal difference (TD) i.e. the difference between the estimated\n",
        "          value of a step and the realized value upon the best move (based\n",
        "          on further estimation of course).\n",
        "  '''\n",
        "  def __init__(self, parameters, alpha, lamda):\n",
        "    self.alpha = alpha\n",
        "    self.lamda = lamda # Note alternate spelling (I didn't make it up!)\n",
        "    defaults   = dict(alpha = alpha, lamda = lamda)\n",
        "\n",
        "    super(TDLambda, self).__init__(parameters, defaults)\n",
        "    \n",
        "  @torch.no_grad()\n",
        "  def step(self, delta, update_trace = True):\n",
        "    '''Performs a single optimization step updating the trace *afterwards*\n",
        "    \n",
        "    update_trace must be called before first step to initialize the trace.\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    \n",
        "    delta: Difference between estimated value of move at time t and realized\n",
        "           value after moving and going to time t+1\n",
        "    '''\n",
        "    \n",
        "    for group in self.param_groups:\n",
        "      alpha = group['alpha']\n",
        "      \n",
        "      for p in group['params']:\n",
        "        if p.grad is None:\n",
        "          continue\n",
        "        \n",
        "        state = self.state[p]\n",
        "        \n",
        "        if len(state) == 0 or 'trace' not in state:\n",
        "          raise RuntimeError('Traces must be initialized before calling step')\n",
        "          \n",
        "        trace = state['trace']\n",
        "        \n",
        "        p.add_(alpha * delta * trace) # Note gradient *ascent* in reinforcement learning\n",
        "    \n",
        "    if update_trace:\n",
        "      self.update_trace()\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def update_trace(self):\n",
        "    '''Updates the trace based on the gradients.\n",
        "    \n",
        "    This also is the only way to initialize the traces.\n",
        "    It must be called after evaluating the starting position\n",
        "    and backprop at the beginning of the game.\n",
        "    '''\n",
        "    for group in self.param_groups:\n",
        "      lamda = group['lamda']\n",
        "      \n",
        "      for p in group['params']:\n",
        "        if p.grad is None:\n",
        "          continue\n",
        "        \n",
        "        state = self.state[p]\n",
        "        \n",
        "        # Initialize to zero if necessary\n",
        "        if len(state) == 0 or 'trace' not in state:\n",
        "          state['trace'] = torch.zeros_like(p)\n",
        "          \n",
        "        trace = state['trace']\n",
        "        trace.mul_(-lamda).add_(p.grad)\n",
        "      \n",
        "  @torch.no_grad()\n",
        "  def zero_trace(self):\n",
        "    for group in self.param_groups:\n",
        "      for p in group['params']:\n",
        "        state = self.state['p']\n",
        "        if 'trace' not in state:\n",
        "          continue\n",
        "          \n",
        "        trace = state['trace']\n",
        "        trace.zero_()\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4L9vL3EwU0E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.utils import Progbar as ProgressBar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1CAfi-4wU0H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def training_loop(optimizer, num_games, off_policy = lambda _ : None):\n",
        "  \n",
        "  initial_state = create_state('H10')\n",
        "  for i in range(num_games):\n",
        "    print(f'\\nPlaying game {i+1} of {num_games}:')\n",
        "    game_loop(initial_state, model, optimizer, off_policy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EptuWzBSwU0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def restart(optimizer, score):\n",
        "  optimizer.zero_trace()\n",
        "  score.backwards()\n",
        "  optimizer.update_trace()\n",
        "  optimizer.zero_grad()\n",
        "  return score.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62GkFibrwU0Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def game_loop(initial_state, model, optimizer, off_policy):\n",
        "  '''Training loop that plays one game'''\n",
        "  # Just in case\n",
        "  optimizer.zero_grad()\n",
        "  \n",
        "  # Initialization\n",
        "  state    = initial_state\n",
        "  score, _ = model(state.unsqueeze(0))\n",
        "  v_t      = restart(optimizer, score)\n",
        "  \n",
        "  # Progress Bar\n",
        "  bar      = ProgressBar(284)\n",
        "  move_num = 1\n",
        "  \n",
        "  while True:\n",
        "    # Determine the next move\n",
        "    game_over, moved_off_policy, new_state, score = \\\n",
        "      get_next_move_training(curr_state, off_policy = off_policy)\n",
        "    \n",
        "    if game_over:      \n",
        "      delta = 1 - v_t\n",
        "      optimizer.step(delta, update_trace = False)\n",
        "      \n",
        "      # Terminate the progress bar\n",
        "      bar.target = move_num\n",
        "      bar.update(move_num)\n",
        "\n",
        "      break\n",
        "    \n",
        "    elif moved_off_policy:\n",
        "      # Equivalent to starting a new game\n",
        "      v_t = restart(optimizer, score)\n",
        "      \n",
        "    else:\n",
        "      score.backwards()\n",
        "      delta = (1 - score) - v_t\n",
        "      optimizer.step(delta)\n",
        "      optimizer.zero_grad()\n",
        "      \n",
        "      v_t   = score.item()\n",
        "      state = new_state\n",
        "      \n",
        "    # Progress bar\n",
        "    move_num += 1\n",
        "    if move_num >= bar.target * 0.9:\n",
        "      bar.target += bar.target // 10 + 1\n",
        "    \n",
        "    bar.update(move_num)\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_Kl9lNTwU0T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_next_move_training(curr_state, off_policy = lambda _ : None):\n",
        "  '''Get the next move for the bot\n",
        "  \n",
        "  Gets the next move for the bot with computations and\n",
        "  return value suitable for training only\n",
        "  (i.e. gradients are taken)\n",
        "  \n",
        "  If off_policy is not None, the off_policy move is\n",
        "  selected instead.\n",
        "  \n",
        "  Gradients with respect to the value-function applied\n",
        "  at the best move are accumlated and availabe to the caller\n",
        "  \n",
        "  Inputs\n",
        "  ------\n",
        "  \n",
        "  curr_state: binary tensor of shape (channels, rows, cols)\n",
        "              reprenting the game state\n",
        "              \n",
        "  off_policy: callable with signature\n",
        "              off_policy(num_available_moves: int) returning\n",
        "              either None or the index of the move desired.\n",
        "              If the return value is not None AND the bot \n",
        "              cannot otherwise win on that move, then that\n",
        "              move is made.\n",
        "              \n",
        "  Outputs\n",
        "  -------\n",
        "  game_over  : boolean. Whether the bot can (and does) win\n",
        "             on this move. The bot *always* plays a\n",
        "             win-in-one move when it is available, regardless\n",
        "             of the off-policy argument.\n",
        "             \n",
        "  off_policy : boolean. Whether an off-policy move was made.\n",
        "             OR: value is None if game is over\n",
        "  \n",
        "  new_state  : a binary tensor of same shape as curr_input\n",
        "             representing the new state of the game after\n",
        "             the bot moves AND the board is flipped around\n",
        "             to present it from opponents view. OR: value\n",
        "             is None if game is over.\n",
        "               \n",
        "  value      : value of the value-function applied to new_state.\n",
        "             OR: value is None if the game is over.\n",
        "  '''\n",
        "  # Compute the placements\n",
        "  placements = get_placements(curr_state)\n",
        "\n",
        "  # Compute the jumps\n",
        "  jumps = get_jumps(curr_state, MAX_JUMPS)\n",
        "\n",
        "  # Deal with special cases/win condition for the jump\n",
        "  \n",
        "  # No jumps to worry about\n",
        "  if len(jumps) == 0:\n",
        "    moves = placements\n",
        "  \n",
        "  # Win condition\n",
        "  elif (\n",
        "    len(jumps) == 1 and\n",
        "    jumps[0][CHAIN][END_LOC][COL] in [config.cols, config.cols-1]):\n",
        "    return True, None, None, None # The game is over!\n",
        "  \n",
        "  # Regular jump evaluation\n",
        "  else:\n",
        "    # Retain only the final state\n",
        "    jumps = [jump_data[0] for jump_data in jumps]\n",
        "    jumps = torch.tensor(jumps, dtype = torch.bool)\n",
        "    moves = torch.cat([placements, jumps])\n",
        "    \n",
        "  # Turn the board around to represent the opponent's view\n",
        "  moves = torch.flip(moves, [-1])\n",
        "  \n",
        "  # Either make an off policy move, or evaluate the value-function\n",
        "  #  to determine the policy\n",
        "  if off_policy(moves.shape[0]) is not None:\n",
        "    move     = moves[move].unsqueeze()\n",
        "    score, _ = model(move)\n",
        "    return False, True, move, score\n",
        "  \n",
        "  # Batch the moves\n",
        "  batches = torch.split(moves, BATCH_SIZE)\n",
        "  \n",
        "  # We only need to differentiate the best score\n",
        "  #  track which one that is.\n",
        "  best_score = None\n",
        "  best_index = None\n",
        "  curr_index = 0\n",
        "\n",
        "  process = psutil.Process(os.getpid())\n",
        "  print(f'{process.memory_info().rss:,d}')\n",
        "  \n",
        "  for batch in batches:\n",
        "\n",
        "    \n",
        "    # Run the model\n",
        "    score, index = model(batch)\n",
        "\n",
        "    # Update running tally of best score\n",
        "    #  Old best scores should have their graphs\n",
        "    #  destroyed\n",
        "\n",
        "    if best_score is None or score > best_score:\n",
        "      best_score = score\n",
        "      best_index = curr_index + index\n",
        "\n",
        "    # Keep track of how many indices we've traversed to \n",
        "    #  get best_index correct\n",
        "    curr_index += batch.shape[0]\n",
        "    \n",
        "    gc.collect()\n",
        "    \n",
        "    print(f'{process.memory_info().rss:,d}')\n",
        "    \n",
        "  # Return\n",
        "  return False, False, moves[best_index], best_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLpIy2PAwU0W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "get_next_move_training(curr_state)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Etz9nnawU0i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Filesystem\n",
    "This notebook is primarily meant to be executed in Colab as a computational backend. If you want to run on your own hardware with data, you need to set `data_dir` and `ALLOW_IO`\n",
    "\n",
    "This notebook viewable directly on Colab from [https://colab.research.google.com/github/rcharan/phutball/blob/rl/pytorch-implementation/model-training.ipynb](https://colab.research.google.com/github/rcharan/phutball/blob/rl/pytorch-implementation/model-training.ipynb) (it is a mirror of github). But if it has moved branches or you are looking at a past commit, look at the [Google instructions](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb) on where to find this file.\n",
    "\n",
    "The workflow is:\n",
    " - Data stored in (my personal/private) Google Drive\n",
    " - Utilities/library files (for importing) on github, edited on local hardware and pushed to github.\n",
    " - Notebook hosted on github, edited both in Colab or locally (depending on the relative value of having a GPU attached versus being able to use regular Jupyter keyboard shortcuts/a superior interface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RY-nqnzUzB5k"
   },
   "outputs": [],
   "source": [
    "# Attempt Colab setup if on Colab\n",
    "try:\n",
    "  import google.colab\n",
    "except:\n",
    "  ALLOW_IO = False\n",
    "else:\n",
    "  # Mount Google Drive at data_dir\n",
    "  #  (for data)\n",
    "  from google.colab import drive\n",
    "  from os.path import join\n",
    "  ROOT = '/content/drive'\n",
    "  DATA = 'My Drive/phutball'\n",
    "  drive.mount(ROOT)\n",
    "  ALLOW_IO = True\n",
    "  data_dir = join(ROOT, DATA)\n",
    "  !mkdir \"{data_dir}\"     # in case we haven't created it already   \n",
    "\n",
    "  # Pull in code from github\n",
    "  %cd /content\n",
    "  github_repo = 'https://github.com/rcharan/phutball'\n",
    "  !git clone -b rl {github_repo}\n",
    "  %cd /content/phutball\n",
    "  \n",
    "  # Point python to code base\n",
    "  import sys\n",
    "  sys.path.append('/content/phutball/pytorch-implementation')\n",
    "\n",
    "  # Updater for library functions changed on local hardware and pushed to github\n",
    "  #  (circuitous, I know)\n",
    "  def update_repo():\n",
    "    !git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yF7BrTwZwUz_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jbaorYhzwUze"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# Codebase\n",
    "from lib.model_v1          import TDConway\n",
    "from lib.moves             import create_placement_getter, get_jumps\n",
    "from lib.utilities         import config\n",
    "from lib.testing_utilities import create_state, visualize_state, boards\n",
    "from lib.timer             import Timer\n",
    "from lib.moves             import END_LOC, COL, CHAIN\n",
    "\n",
    "# Graphics for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "plt.ioff()\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "# Tensorflow solely for the Progress Bar (totally worth it)\n",
    "from tensorflow.keras.utils import Progbar as ProgressBar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Management Utilities\n",
    "Setup for GPU, CPU, or (not working well/fully implemented) TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.00GiB available memory on CPU-based runtime\n"
     ]
    }
   ],
   "source": [
    "use_tpu = False\n",
    "\n",
    "if use_tpu:\n",
    "  # Install PyTorch/XLA\n",
    "  !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
    "  !python pytorch-xla-env-setup.py --version $VERSION\n",
    "  import torch_xla\n",
    "  import torch_xla.core.xla_model as xm\n",
    "  \n",
    "  # Set the device\n",
    "  device = xm.xla_device()\n",
    "  \n",
    "  # Memory inspection\n",
    "  def print_memory_usage():\n",
    "    print('TPU memory inspection not implemented')\n",
    "  def print_max_memory_usage():\n",
    "    print('TPU memory inspection not implemented')\n",
    "  def garbage_collect():\n",
    "    gc.collect() # No TPU specific implementation yet\n",
    "    \n",
    "elif torch.cuda.is_available():\n",
    "  # Set the device\n",
    "  device = torch.device('cuda')\n",
    "  \n",
    "  # Echo GPU info\n",
    "  gpu_info = !nvidia-smi\n",
    "  gpu_info = '\\n'.join(gpu_info)\n",
    "  print(gpu_info)\n",
    "  \n",
    "  # Memory inspection and management\n",
    "  from lib.memory import (\n",
    "    print_memory_usage_cuda     as print_memory_usage,\n",
    "    print_max_memory_usage_cuda as print_max_memory_usage,\n",
    "    garbage_collect_cuda        as garbage_collect\n",
    "  )\n",
    "\n",
    "else:\n",
    "  # Set the device to CPU\n",
    "  device = torch.device('cpu')\n",
    "  \n",
    "  # Echo RAM info\n",
    "  from psutil import virtual_memory\n",
    "  from lib.memory import format_bytes\n",
    "  ram = virtual_memory().total\n",
    "  print(format_bytes(ram), 'available memory on CPU-based runtime')\n",
    "  \n",
    "  # Memory inspection and management\n",
    "  from lib.memory import (\n",
    "    print_memory_usage, \n",
    "    print_max_memory_usage,\n",
    "    garbage_collect\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kl9_Q1WNwUz8"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "MAX_JUMPS = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tools\n",
    "These tools will be moved to .py files in the lib folder when stable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelState:\n",
    "\n",
    "  def __init__(self, model_name, model = None,\n",
    "               target_ensemble = None, \n",
    "               optimizer = None, optimizer_class = Adam,\n",
    "               epochs_done = 0, \n",
    "               best_val_loss = float('inf'),\n",
    "               train_tensor = None, valid_tensor = None, valid_df = None,\n",
    "               other_data = None):\n",
    "    self.model_name      = model_name\n",
    "    self.model           = model\n",
    "    self.target_ensemble = target_ensemble\n",
    "    self.epochs_done     = epochs_done\n",
    "    self.optimizer       = optimizer\n",
    "    self.optimizer_class = optimizer_class\n",
    "    self.best_val_loss   = best_val_loss\n",
    "    self.other_data      = other_data\n",
    "    self.train_tensor    = train_tensor\n",
    "    self.valid_tensor    = valid_tensor\n",
    "    self.valid_df        = valid_df\n",
    "    \n",
    "  def update(self, epochs_done, best_val_loss, other_data):\n",
    "    self.epochs_done   = epochs_done\n",
    "    self.best_val_loss = best_val_loss\n",
    "    self.other_data    = other_data\n",
    "\n",
    "  @property\n",
    "  def fname(self):\n",
    "    return  f'penn/checkpoints/{self.model_name}.pt'\n",
    "\n",
    "  def temp_fname(self, subdir, model_name, info):\n",
    "    return f'penn/checkpoints/{subdir}/{model_name}/{info}.pt'\n",
    "\n",
    "  @property\n",
    "  def trainable_params(self):\n",
    "    return [\n",
    "      name \n",
    "      for name, param in self.model.named_parameters()\n",
    "      if  param.requires_grad\n",
    "    ]\n",
    "\n",
    "  def state_dict(self):\n",
    "    return {\n",
    "        'model'            : self.model.state_dict(),\n",
    "        'games_done'       : self.epochs_done,\n",
    "        'optimizer'        : self.optimizer.state_dict(),\n",
    "        'game_lengths'     : self.game_lengths\n",
    "        'dropout'          : self.model.dropout,\n",
    "        'trainable_params' : self.trainable_params,\n",
    "    }\n",
    "\n",
    "  def save_head(self, target, epoch):\n",
    "    param = self.model.heads[target.col_name]\n",
    "    fname = self.temp_fname('heads', self.model_name, f'{target.col_name}-{epoch}')\n",
    "    with open_safely(fname, 'wb') as f:\n",
    "      torch.save(param.state_dict(), f)\n",
    "\n",
    "  def register_parent_model(self, names):\n",
    "    self.parent_models = names\n",
    "\n",
    "  def load_head(self, target):\n",
    "    param = self.model.heads[target.col_name]\n",
    "    model_names = self.parent_models.copy()\n",
    "    def get_fname(model_base):\n",
    "      return self.temp_fname('heads', model_base,\n",
    "                              f'{target.col_name}-{target.best_val_epoch}')\n",
    "    fname = get_fname(self.model_name)\n",
    "    while not os.path.exists(fname):\n",
    "      # print(f\"Can't find {fname}\")\n",
    "      try:\n",
    "        model_name = model_names.pop()\n",
    "      except IndexError:\n",
    "        print(f'Unable to load {target.col_name} at epoch {target.best_val_epoch}')\n",
    "        return\n",
    "      fname = get_fname(model_name)\n",
    "      # print(f\"Trying {fname}\")\n",
    "\n",
    "\n",
    "    with open_safely(fname, 'rb') as f:\n",
    "      sd = torch.load(f)\n",
    "    \n",
    "    param.load_state_dict(sd)\n",
    "\n",
    "  def save(self):\n",
    "    with open_safely(self.fname, 'wb') as f:\n",
    "      torch.save(self.state_dict(), f)\n",
    "\n",
    "  def load(self, initial_state = lambda _ : None):\n",
    "    path = self.fname\n",
    "    if os.path.exists(path):\n",
    "      print(f'Restoring from {path}')\n",
    "    else:\n",
    "      print(f'No saved model found, proceeding with a fresh initialization')\n",
    "      return initial_state(self.model_name)\n",
    "\n",
    "    with open_safely(self.fname, 'rb') as f:\n",
    "      sd = torch.load(f, map_location = torch.device('cpu'))\n",
    "\n",
    "    self.target_ensemble = TargetEnsemble([])\n",
    "    self.target_ensemble.load_state_dict(sd['targets'])\n",
    "\n",
    "    self.model = BERT(sd['dropout'], self.target_ensemble)\n",
    "    self.model.load_state_dict(sd['model'])\n",
    "    self.model = self.model.to(device)\n",
    "\n",
    "    for name, param in self.model.named_parameters():\n",
    "      if name in sd['trainable_params']:\n",
    "        param.requires_grad_(True)\n",
    "      else:\n",
    "        param.requires_grad_(False)\n",
    "\n",
    "    trainable_params = lfilter(lambda param : param.requires_grad, self.model.parameters())\n",
    "\n",
    "    self.optimizer = self.optimizer_class(trainable_params)\n",
    "    self.optimizer.load_state_dict(sd['optimizer'])\n",
    "\n",
    "    self.best_val_loss = sd['best_val_loss']\n",
    "    self.epochs_done   = sd['epochs_done']\n",
    "    self.train_tensor  = sd['train_tensor']\n",
    "    self.valid_tensor  = sd['valid_tensor']\n",
    "    self.other_data    = sd['other_data']\n",
    "    self.valid_df      = sd['valid_df']\n",
    "\n",
    "    self.train_tensor = lmap(lambda t : t.to(device), self.train_tensor)\n",
    "    self.valid_tensor = lmap(lambda t : t.to(device), self.valid_tensor)\n",
    "\n",
    "    print(f'{self.epochs_done} epochs run with'\n",
    "          f' best validation loss {self.best_val_loss:.2f}')\n",
    "    \n",
    "    return self\n",
    "    \n",
    "  def inspect(self):\n",
    "    if not os.path.exists(self.fname):\n",
    "      print('No such model')\n",
    "      return\n",
    "\n",
    "    state_dict = torch.load(self.fname, map_location = torch.device('cpu'))\n",
    "    print(f'''Epochs run: {state_dict['epochs_done']}\\n'''\n",
    "          f'''Valid loss: {state_dict['best_val_loss']:.3f}''')\n",
    "\n",
    "  def size_on_disk(self):\n",
    "    if not os.path.exists(self.fname):\n",
    "      return '0'\n",
    "    else:\n",
    "      return format_bytes(os.path.getsize(self.fname))\n",
    "  \n",
    "  @classmethod\n",
    "  def _dataset(self, tensors, batch_size, shuffle = True):\n",
    "    # Construct a Torch Dataset, then a DataLoader that random samples and batches\n",
    "    dataset     = TensorDataset(*tensors)\n",
    "    dataset     = DataLoader(dataset, batch_size, shuffle = shuffle)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "  def train_dataset(self, batch_size):\n",
    "    return self._dataset(self.train_tensor, batch_size, shuffle = True)\n",
    "\n",
    "  def valid_dataset(self, batch_size):\n",
    "    return self._dataset(self.valid_tensor, batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rkzXSlsbwU0C"
   },
   "outputs": [],
   "source": [
    "class AlternatingTDLambda(Optimizer):\n",
    "  '''Implements tracing and updates for the TD(λ) algorithm.\n",
    "  \n",
    "  For details see Sutton and Barto, Reinforcement Learning 2ed,\n",
    "  Chaper 12 Section 2.\n",
    "  \n",
    "  Modifications:\n",
    "    (1) the algorithm is modified to compute traces\n",
    "        one step early (when the graph is available). Hence, trace must be\n",
    "        initialized with update_trace before calling step.\n",
    "        \n",
    "    (2) Because the board switches sides each turn, eligibility trace updates\n",
    "        must be of the form z_t+1 <-  -λz_t + 𝝯v\n",
    "  \n",
    "  You may find the following greeks a helpful reference:\n",
    "  alpha : Learning Rate\n",
    "  lambda: Exponential Decay parameter for the eligibility trace\n",
    "          (which is essentially momentum but with different\n",
    "           interpertation and is occasionally zeroed out).\n",
    "  delta : Temporal difference (TD) i.e. the difference between the estimated\n",
    "          value of a step and the realized value upon the best move (based\n",
    "          on further estimation of course).\n",
    "  '''\n",
    "  def __init__(self, parameters, alpha, lamda):\n",
    "    self.alpha = alpha\n",
    "    self.lamda = lamda # Note alternate spelling (I didn't make it up!)\n",
    "    defaults   = dict(alpha = alpha, lamda = lamda)\n",
    "\n",
    "    super(TDLambda, self).__init__(parameters, defaults)\n",
    "    \n",
    "  @torch.no_grad()\n",
    "  def step(self, delta, update_trace = True):\n",
    "    '''Performs a single optimization step updating the trace *afterwards*\n",
    "    \n",
    "    update_trace must be called before first step to initialize the trace.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    \n",
    "    delta: Difference between estimated value of move at time t and realized\n",
    "           value after moving and going to time t+1\n",
    "    '''\n",
    "    \n",
    "    for group in self.param_groups:\n",
    "      alpha = group['alpha']\n",
    "      \n",
    "      for p in group['params']:\n",
    "        if p.grad is None:\n",
    "          continue\n",
    "        \n",
    "        state = self.state[p]\n",
    "        \n",
    "        if len(state) == 0 or 'trace' not in state:\n",
    "          raise RuntimeError('Traces must be initialized before calling step')\n",
    "          \n",
    "        trace = state['trace']\n",
    "        \n",
    "        p.add_(alpha * delta * trace) # Note gradient *ascent* in reinforcement learning\n",
    "    \n",
    "    if update_trace:\n",
    "      self.update_trace()\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def update_trace(self):\n",
    "    '''Updates the trace based on the gradients.\n",
    "    \n",
    "    This also is the only way to initialize the traces.\n",
    "    It must be called after evaluating the starting position\n",
    "    and backprop at the beginning of the game.\n",
    "    '''\n",
    "    for group in self.param_groups:\n",
    "      lamda = group['lamda']\n",
    "      \n",
    "      for p in group['params']:\n",
    "        if p.grad is None:\n",
    "          continue\n",
    "        \n",
    "        state = self.state[p]\n",
    "        \n",
    "        # Initialize to zero if necessary\n",
    "        if len(state) == 0 or 'trace' not in state:\n",
    "          state['trace'] = torch.zeros_like(p)\n",
    "          \n",
    "        trace = state['trace']\n",
    "        trace.mul_(-lamda).add_(p.grad)\n",
    "      \n",
    "  @torch.no_grad()\n",
    "  def zero_trace(self):\n",
    "    for group in self.param_groups:\n",
    "      for p in group['params']:\n",
    "        state = self.state['p']\n",
    "        if 'trace' not in state:\n",
    "          continue\n",
    "          \n",
    "        trace = state['trace']\n",
    "        trace.zero_()\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p1CAfi-4wU0H"
   },
   "outputs": [],
   "source": [
    "def training_loop(optimizer, num_games, off_policy = lambda _ : None):\n",
    "  \n",
    "  initial_state = create_state('H10').to(device)\n",
    "  for i in range(num_games):\n",
    "    print(f'\\nPlaying game {i+1} of {num_games}:')\n",
    "    game_loop(initial_state, model, optimizer, off_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EptuWzBSwU0N"
   },
   "outputs": [],
   "source": [
    "def restart(optimizer, score):\n",
    "  optimizer.zero_trace()\n",
    "  score.backwards()\n",
    "  optimizer.update_trace()\n",
    "  optimizer.zero_grad()\n",
    "  return score.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "62GkFibrwU0Q"
   },
   "outputs": [],
   "source": [
    "def game_loop(initial_state, model, optimizer, off_policy):\n",
    "  '''Training loop that plays one game'''\n",
    "  # Just in case\n",
    "  optimizer.zero_grad()\n",
    "  \n",
    "  # Initialization\n",
    "  state    = initial_state\n",
    "  score, _ = model(state.unsqueeze(0))\n",
    "  v_t      = restart(optimizer, score)\n",
    "  \n",
    "  # Progress Bar\n",
    "  bar      = ProgressBar(284)\n",
    "  move_num = 1\n",
    "  \n",
    "  while True:\n",
    "    # Determine the next move\n",
    "    game_over, moved_off_policy, new_state, score = \\\n",
    "      get_next_move_training(curr_state, off_policy = off_policy)\n",
    "    \n",
    "    if game_over:      \n",
    "      delta = 1 - v_t\n",
    "      optimizer.step(delta, update_trace = False)\n",
    "      \n",
    "      # Terminate the progress bar\n",
    "      bar.target = move_num\n",
    "      bar.update(move_num)\n",
    "\n",
    "      break\n",
    "    \n",
    "    elif moved_off_policy:\n",
    "      # Equivalent to starting a new game\n",
    "      v_t = restart(optimizer, score)\n",
    "      \n",
    "    else:\n",
    "      score.backwards()\n",
    "      delta = (1 - score) - v_t\n",
    "      optimizer.step(delta)\n",
    "      optimizer.zero_grad()\n",
    "      \n",
    "      v_t   = score.item()\n",
    "      state = new_state\n",
    "      \n",
    "    # Progress bar\n",
    "    move_num += 1\n",
    "    if move_num >= bar.target * 0.9:\n",
    "      bar.target += bar.target // 10 + 1\n",
    "    \n",
    "    bar.update(move_num)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2_Kl9lNTwU0T"
   },
   "outputs": [],
   "source": [
    "def get_next_move_training(curr_state, off_policy = lambda _ : None):\n",
    "  '''Get the next move for the bot\n",
    "  \n",
    "  Gets the next move for the bot with computations and\n",
    "  return value suitable for training only\n",
    "  (i.e. gradients are taken)\n",
    "  \n",
    "  If off_policy is not None, the off_policy move is\n",
    "  selected instead.\n",
    "  \n",
    "  Gradients with respect to the value-function applied\n",
    "  at the best move are accumlated and availabe to the caller\n",
    "  \n",
    "  Inputs\n",
    "  ------\n",
    "  \n",
    "  curr_state: binary tensor of shape (channels, rows, cols)\n",
    "              reprenting the game state\n",
    "              \n",
    "  off_policy: callable with signature\n",
    "              off_policy(num_available_moves: int) returning\n",
    "              either None or the index of the move desired.\n",
    "              If the return value is not None AND the bot \n",
    "              cannot otherwise win on that move, then that\n",
    "              move is made.\n",
    "              \n",
    "  Outputs\n",
    "  -------\n",
    "  game_over  : boolean. Whether the bot can (and does) win\n",
    "             on this move. The bot *always* plays a\n",
    "             win-in-one move when it is available, regardless\n",
    "             of the off-policy argument.\n",
    "             \n",
    "  off_policy : boolean. Whether an off-policy move was made.\n",
    "             OR: value is None if game is over\n",
    "  \n",
    "  new_state  : a binary tensor of same shape as curr_input\n",
    "             representing the new state of the game after\n",
    "             the bot moves AND the board is flipped around\n",
    "             to present it from opponents view. OR: value\n",
    "             is None if game is over.\n",
    "               \n",
    "  value      : value of the value-function applied to new_state.\n",
    "             OR: value is None if the game is over.\n",
    "  '''\n",
    "  # Compute the placements\n",
    "  placements = get_placements(curr_state)\n",
    "\n",
    "  # Compute the jumps\n",
    "  jumps = get_jumps(curr_state, MAX_JUMPS)\n",
    "\n",
    "  # Deal with special cases/win condition for the jump\n",
    "  \n",
    "  # No jumps to worry about\n",
    "  if len(jumps) == 0:\n",
    "    moves = placements\n",
    "  \n",
    "  # Win condition\n",
    "  elif (\n",
    "    len(jumps) == 1 and\n",
    "    jumps[0][CHAIN][END_LOC][COL] in [config.cols, config.cols-1]):\n",
    "    return True, None, None, None # The game is over!\n",
    "  \n",
    "  # Regular jump evaluation\n",
    "  else:\n",
    "    # Retain only the final state\n",
    "    jumps = [jump_data[0] for jump_data in jumps]\n",
    "    jumps = torch.tensor(jumps, dtype = torch.bool)\n",
    "    moves = torch.cat([placements, jumps])\n",
    "    \n",
    "  # Turn the board around to represent the opponent's view\n",
    "  moves = torch.flip(moves, [-1])\n",
    "  \n",
    "  # Either make an off policy move, or evaluate the value-function\n",
    "  #  to determine the policy\n",
    "  if off_policy(moves.shape[0]) is not None:\n",
    "    move     = moves[move].unsqueeze()\n",
    "    score, _ = model(move)\n",
    "    return False, True, move, score\n",
    "  \n",
    "  # Batch the moves\n",
    "  batches = torch.split(moves, BATCH_SIZE)\n",
    "  \n",
    "  # We only need to differentiate the best score\n",
    "  #  track which one that is.\n",
    "  best_score = None\n",
    "  best_index = None\n",
    "  curr_index = 0\n",
    "\n",
    "  \n",
    "  timer = Timer()\n",
    "  for batch in batches:\n",
    "\n",
    "    \n",
    "    # Run the model\n",
    "    score, index = model(batch)\n",
    "\n",
    "    # Update running tally of best score\n",
    "    #  Old best scores should have their graphs\n",
    "    #  destroyed\n",
    "\n",
    "    if best_score is None or score > best_score:\n",
    "      best_score = score\n",
    "      best_index = curr_index + index\n",
    "\n",
    "    # Keep track of how many indices we've traversed to \n",
    "    #  get best_index correct\n",
    "    curr_index += batch.shape[0]\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    print(timer)\n",
    "    # print(f'{process.memory_info().rss:,d}')\n",
    "    \n",
    "  # Return\n",
    "  return False, False, moves[best_index], best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EBpw7Ul8wUzs"
   },
   "outputs": [],
   "source": [
    "get_placements = create_placement_getter(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TKTjg7fcwUzv"
   },
   "outputs": [],
   "source": [
    "model = TDConway(config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "bk_MuwokwUzz",
    "outputId": "2d1aeba1-6790-4f4c-bf7e-e8a6cca0c9d7"
   },
   "outputs": [],
   "source": [
    "initial_state = create_state('H10').to(device)\n",
    "visualize_state(initial_state.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6DwZFYQAwUz3"
   },
   "outputs": [],
   "source": [
    "curr_state = initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "colab_type": "code",
    "id": "-Etz9nnawU0i",
    "outputId": "646b3347-ee2f-4cc5-a2d8-6bc1680dc8d8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "get_next_move_training(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "model-training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utilities import config, product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module, ModuleList, Conv2d, SELU, MaxPool2d, Sequential, Linear, Flatten, Dropout, Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConvStack(Module):\n",
    "\n",
    "  def __init__(self, kernel_size, conv_depth, layer_structure = [1,2], initial_depth = None, activation = SELU):\n",
    "    '''Convolution Stack with Residual Structure.\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    \n",
    "    kernel_size    : as in Conv2d\n",
    "    conv_depth     : output depth\n",
    "    layer_strucutre: list of ints. The first int represents the number of convolutions\n",
    "                     to apply initially. After that, each int represents a number of\n",
    "                     convolutions to apply before adding the residual from the previous\n",
    "                     state. The default [1,2] does 1 convolution to output \"x\" and then \n",
    "                     does two convolutions and adds x\n",
    "    initial_depth  : depth of the first input (defaults to conv_depth)\n",
    "    activation     : class for activation\n",
    "    \n",
    "    '''\n",
    "    super(ResidualConvStack, self).__init__()\n",
    "    \n",
    "    if initial_depth is None:\n",
    "      initial_depth = conv_depth\n",
    "    \n",
    "    self.convs = ModuleList([])\n",
    "    self.convs.append(Conv2d(initial_depth, conv_depth, kernel_size, padding = kernel_size // 2))\n",
    "    \n",
    "    for _ in range(sum(layer_structure) - 1):\n",
    "      self.convs.append(Conv2d(conv_depth, conv_depth, kernel_size, padding = kernel_size // 2))\n",
    "    \n",
    "    self.layer_structure = layer_structure\n",
    "    self.activation      = activation()\n",
    "    \n",
    "  def forward(self, signal):\n",
    "    num_applied = 0\n",
    "    for i, num_layers in enumerate(self.layer_structure):\n",
    "      to_apply     = self.convs[num_applied : num_applied + num_layers]\n",
    "      num_applied += num_layers\n",
    "      \n",
    "      resid_signal = signal\n",
    "      \n",
    "      for j, layer in enumerate(to_apply):\n",
    "        signal = layer(signal)\n",
    "        if j != num_layers - 1:\n",
    "          signal = self.activation(signal)\n",
    "        \n",
    "      if i != 0:\n",
    "        signal = resid_signal + signal\n",
    "        \n",
    "      signal = self.activation(signal)\n",
    "      \n",
    "    return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDConway(Module):\n",
    "  \n",
    "  def __init__(self, config, dropout = 0.2):\n",
    "    super(TDConway, self).__init__()\n",
    "    \n",
    "    self.stack_1 = ModuleList([\n",
    "      ResidualConvStack(3, 64, layer_structure = [1,2,2], initial_depth = config.num_channels),\n",
    "      ResidualConvStack(5, 64, layer_structure = [1,2,2], initial_depth = config.num_channels),\n",
    "    ])\n",
    "    \n",
    "    self.stack_2 = ModuleList([\n",
    "      ResidualConvStack(1, 64 * 2, layer_structure = [0, 2, 2]),\n",
    "      ResidualConvStack(3, 64 * 2, layer_structure = [0, 2, 2]),\n",
    "      ResidualConvStack(5, 64 * 2, layer_structure = [0, 2, 2]),\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    self.fc = Sequential(\n",
    "      Flatten(), \n",
    "      Linear(64 * 2 * 3 * config.rows * config.cols, 512),\n",
    "      SELU(),\n",
    "      Dropout(dropout),\n",
    "      Linear(512, 2048),\n",
    "      SELU(),\n",
    "      Dropout(dropout),\n",
    "      Linear(2048, 1),\n",
    "      Sigmoid()\n",
    "    )\n",
    "    \n",
    "  def forward(self, signal, get_all_values = False):\n",
    "    signal = signal.float()\n",
    "    \n",
    "    signal = torch.cat(\n",
    "      [conv_stack(signal) for conv_stack in self.stack_1],\n",
    "      dim = 1\n",
    "    )\n",
    "    \n",
    "    signal = torch.cat(\n",
    "      [conv_stack(signal) for conv_stack in self.stack_2],\n",
    "      dim = 1\n",
    "    )\n",
    "    \n",
    "    signal = self.fc(signal)\n",
    "    \n",
    "    values = signal.squeeze()\n",
    "        \n",
    "    if get_all_values:\n",
    "      return values\n",
    "    else:\n",
    "      (best_value, best_index) = torch.max(values, 0)\n",
    "      return best_value, best_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_params(model, printIt = True):\n",
    "  out = sum(product(p.shape) for p in model.parameters())\n",
    "  print(f'{out:,d} parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59,943,809 parameters\n"
     ]
    }
   ],
   "source": [
    "get_num_params(TDConway(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_param_num_details(model):\n",
    "  for n, p in model.named_parameters():\n",
    "    print(f'{product(p.shape):7,d}', n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 2, 15, 19])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lib.testing_utilities import boards\n",
    "boards = torch.stack(boards)\n",
    "boards.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_conway = TDConway(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.23 s ± 74.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "with torch.no_grad():\n",
    "  td_conway(boards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.5302), tensor(4))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  res = td_conway(boards)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
